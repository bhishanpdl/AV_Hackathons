{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Description</a></span></li><li><span><a href=\"#Load-the-libraries\" data-toc-modified-id=\"Load-the-libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load the libraries</a></span></li><li><span><a href=\"#Load-the-data\" data-toc-modified-id=\"Load-the-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load the data</a></span></li><li><span><a href=\"#Useful-Functions\" data-toc-modified-id=\"Useful-Functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Useful Functions</a></span></li><li><span><a href=\"#Text-Data-Processing\" data-toc-modified-id=\"Text-Data-Processing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Text Data Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Process-text\" data-toc-modified-id=\"Process-text-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Process text</a></span></li><li><span><a href=\"#Emoticons-and-emojis\" data-toc-modified-id=\"Emoticons-and-emojis-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Emoticons and emojis</a></span></li></ul></li><li><span><a href=\"#Text-Features-Generation\" data-toc-modified-id=\"Text-Features-Generation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Text Features Generation</a></span></li><li><span><a href=\"#Script\" data-toc-modified-id=\"Script-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Script</a></span></li><li><span><a href=\"#Script-for-emoji\" data-toc-modified-id=\"Script-for-emoji-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Script for emoji</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Twitter sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:24:22.271773Z",
     "start_time": "2020-09-01T01:24:22.267653Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/poudel/opt/miniconda3/envs/nlp/lib/python3.7/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:31:32.345392Z",
     "start_time": "2020-09-01T01:31:32.339155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nltk', '3.4.4'), ('spacy', '2.2.3'), ('textblob', '0.15.3'), ('gensim', '3.8.3')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',1000)\n",
    "\n",
    "#========= NLP\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import textblob\n",
    "import gensim\n",
    "import texthero\n",
    "from urllib.parse import urlparse\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print([(x.__name__,x.__version__) for x in [nltk,spacy,textblob,gensim]])\n",
    "\n",
    "#=======OTHERS\n",
    "import scipy\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T16:15:20.695546Z",
     "start_time": "2020-08-31T16:15:20.688345Z"
    }
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:55:16.881299Z",
     "start_time": "2020-09-01T01:55:16.856940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape df_train_raw: (50, 3)\n",
      "shape df_test_raw: (50, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train_raw = pd.read_csv('../data/raw/train.csv',nrows=50)\n",
    "df_test_raw = pd.read_csv('../data/raw/test.csv',nrows=50)\n",
    "\n",
    "df = df_train_raw.append(df_test_raw)\n",
    "df = df.reset_index()\n",
    "\n",
    "print(f\"shape df_train_raw: {df_train_raw.shape}\")\n",
    "print(f\"shape df_test_raw: {df_test_raw.shape}\")\n",
    "\n",
    "df.head(2).append(df.tail(2))\n",
    "\n",
    "target = 'label'\n",
    "maincol = 'tweet'\n",
    "mc = maincol + '_clean'\n",
    "mcl = maincol + '_lst_clean'\n",
    "mce = mc + '_emoji'\n",
    "mcle = mcl + '_emoji'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:31:45.097509Z",
     "start_time": "2020-09-01T01:31:45.093208Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "def parallelize_dataframe(df, func):\n",
    "    ncores = mp.cpu_count()\n",
    "    df_split = np.array_split(df, ncores)\n",
    "    pool = mp.Pool(ncores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:31:46.745230Z",
     "start_time": "2020-09-01T01:31:46.741805Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "def is_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:31:48.729085Z",
     "start_time": "2020-09-01T01:31:48.700007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['retwet', 'textnum', 'yes', 'god', 'wwxycom', 'amazing']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Do a basic text processing.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    text : string\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    This function returns pandas series having one list\n",
    "    with clean text.\n",
    "    1: split combined text\n",
    "    2: lowercase\n",
    "    3: expand apostrophes\n",
    "    4: remove punctuation\n",
    "    5: remove digits\n",
    "    6: remove repeated substring\n",
    "    7: remove stop words\n",
    "    8: lemmatize\n",
    "\n",
    "    Example:\n",
    "    ========\n",
    "    import re\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    \n",
    "    text = \"I'm typing text2num! areYou ? If yesyes say yes pals!\"\n",
    "    process_text(text)\n",
    "    # ['typing', 'textnum', 'yes', 'say', 'yes', 'pal']\n",
    "\n",
    "    \"\"\"\n",
    "    s = pd.Series([text])\n",
    "    \n",
    "    # step: Split combined words areYou ==> are You\n",
    "    #s = s.apply(lambda x: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',x))\n",
    "\n",
    "    # step: lowercase\n",
    "    s = s.str.lower()\n",
    "    \n",
    "    # step: remove ellipsis\n",
    "    #s = s.str.replace(r'(\\w)\\u2026+',r'\\1',regex=True)\n",
    "    s = s.str.replace(r'â€¦+',r'')\n",
    "\n",
    "    # step: remove url\n",
    "    #s = s.str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    s = pd.Series([' '.join(y for y in x.split() if not is_url(y)) for x in s])\n",
    "\n",
    "    # step: expand apostrophes\n",
    "    map_apos = {\n",
    "        \"you're\": 'you are',\n",
    "        \"i'm\": 'i am',\n",
    "        \"he's\": 'he is',\n",
    "        \"she's\": 'she is',\n",
    "        \"it's\": 'it is',\n",
    "        \"they're\": 'they are',\n",
    "        \"can't\": 'can not',\n",
    "        \"couldn't\": 'could not',\n",
    "        \"don't\": 'do not',\n",
    "        \"don;t\": 'do not',\n",
    "        \"didn't\": 'did not',\n",
    "        \"doesn't\": 'does not',\n",
    "        \"isn't\": 'is not',\n",
    "        \"wasn't\": 'was not',\n",
    "        \"aren't\": 'are not',\n",
    "        \"weren't\": 'were not',\n",
    "        \"won't\": 'will not',\n",
    "        \"wouldn't\": 'would not',\n",
    "        \"hasn't\": 'has not',\n",
    "        \"haven't\": 'have not',\n",
    "        \"what's\": 'what is',\n",
    "        \"that's\": 'that is',\n",
    "    }\n",
    "\n",
    "    sa = pd.Series(s.str.split()[0])\n",
    "    sb = sa.map(map_apos).fillna(sa)\n",
    "    sentence = sb.str.cat(sep=' ')\n",
    "    s = pd.Series([sentence])\n",
    "    \n",
    "    # step: expand shortcuts\n",
    "    shortcuts = {'u': 'you', 'y': 'why', 'r': 'are',\n",
    "                 'doin': 'doing', 'hw': 'how',\n",
    "                 'k': 'okay', 'm': 'am', 'b4': 'before',\n",
    "                 'idc': \"i do not care\", 'ty': 'thankyou',\n",
    "                 'wlcm': 'welcome', 'bc': 'because',\n",
    "                 '<3': 'love', 'xoxo': 'love',\n",
    "                 'ttyl': 'talk to you later', 'gr8': 'great',\n",
    "                 'bday': 'birthday', 'awsm': 'awesome',\n",
    "                 'gud': 'good', 'h8': 'hate',\n",
    "                 'lv': 'love', 'dm': 'direct message',\n",
    "                 'rt': 'retweet', 'wtf': 'hate',\n",
    "                 'idgaf': 'hate','irl': 'in real life',\n",
    "                 'yolo': 'you only live once'}\n",
    "\n",
    "    sa = pd.Series(s.str.split()[0])\n",
    "    sb = sa.map(shortcuts).fillna(sa)\n",
    "    sentence = sb.str.cat(sep=' ')\n",
    "    s = pd.Series([sentence])\n",
    "\n",
    "    # step: remove punctuation\n",
    "    s = s.str.translate(str.maketrans(' ',' ',\n",
    "                                        string.punctuation))\n",
    "    # step: remove digits\n",
    "    s = s.str.translate(str.maketrans(' ', ' ', '\\n'))\n",
    "    s = s.str.translate(str.maketrans(' ', ' ', string.digits))\n",
    "\n",
    "    # step: remove repeated substring yesyes ==> yes\n",
    "    s = s.str.replace(r'(\\w+)\\1',r'\\1',regex=True)\n",
    "\n",
    "    # step: remove stop words\n",
    "    stop = set(stopwords.words('English'))\n",
    "    extra_stop_words = ['...']\n",
    "    stop.update(extra_stop_words) # inplace operation\n",
    "    s = s.str.split()\n",
    "    s = s.apply(lambda x: [I for I in x if I not in stop])\n",
    "\n",
    "    # step: convert word to base form or lemmatize\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    s = s.apply(lambda lst: [lemmatizer.lemmatize(word) \n",
    "                               for word in lst])\n",
    "\n",
    "    return s.to_numpy()[0]\n",
    "\n",
    "text = \"rt text2num! yesyes gud www.xy.com amazing\"\n",
    "process_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:32:26.573758Z",
     "start_time": "2020-09-01T01:32:26.568556Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df[mcl] = df[maincol].apply(process_text)\n",
    "    df[mc] = df[mcl].str.join(' ')\n",
    "    df['hashtags_lst'] = df[maincol].str.findall(r'#.*?(?=\\s|$)')\n",
    "    \n",
    "    #df['hastags'] = df[mc].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "    \n",
    "    df['hashtags'] = df['hashtags_lst'].str.join(' ')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:32:29.013594Z",
     "start_time": "2020-09-01T01:32:28.661609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 ms, sys: 22.8 ms, total: 39.3 ms\n",
      "Wall time: 346 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = parallelize_dataframe(df, add_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:32:31.007393Z",
     "start_time": "2020-09-01T01:32:30.990484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_lst_clean</th>\n",
       "      <th>tweet_clean</th>\n",
       "      <th>hashtags_lst</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "      <td>[fingerprint, pregnancy, test, android, aps, beautiful, cute, health, igers, iphoneonly, iphonesia, iphone]</td>\n",
       "      <td>fingerprint pregnancy test android aps beautiful cute health igers iphoneonly iphonesia iphone</td>\n",
       "      <td>[#fingerprint, #Pregnancy, #android, #apps, #beautiful, #cute, #health, #igers, #iphoneonly, #iphonesia, #iphone]</td>\n",
       "      <td>#fingerprint #Pregnancy #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperiasâ€¦ http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "      <td>[finaly, transparant, silicon, case, thanks, uncle, yay, sony, xperia, sonyexperias]</td>\n",
       "      <td>finaly transparant silicon case thanks uncle yay sony xperia sonyexperias</td>\n",
       "      <td>[#yay, #Sony, #Xperia, #S, #sonyexperiasâ€¦]</td>\n",
       "      <td>#yay #Sony #Xperia #S #sonyexperiasâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu</td>\n",
       "      <td>[love, would, go, talk, makemories, unplug, relax, iphone, smartphone, wifi, conect]</td>\n",
       "      <td>love would go talk makemories unplug relax iphone smartphone wifi conect</td>\n",
       "      <td>[#talk, #makememories, #unplug, #relax, #iphone, #smartphone, #wifi, #connect...]</td>\n",
       "      <td>#talk #makememories #unplug #relax #iphone #smartphone #wifi #connect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/</td>\n",
       "      <td>[wired, know, george, made, way, iphone, cute, daventry, home]</td>\n",
       "      <td>wired know george made way iphone cute daventry home</td>\n",
       "      <td>[#iphone, #cute, #daventry, #home]</td>\n",
       "      <td>#iphone #cute #daventry #home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!</td>\n",
       "      <td>[amazing, service, aple, wil, even, talk, question, unles, pay, stupid, suport]</td>\n",
       "      <td>amazing service aple wil even talk question unles pay stupid suport</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id  label  \\\n",
       "0      0   1    0.0   \n",
       "1      1   2    0.0   \n",
       "2      2   3    0.0   \n",
       "3      3   4    0.0   \n",
       "4      4   5    1.0   \n",
       "\n",
       "                                                                                                                                 tweet  \\\n",
       "0     #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone   \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperiasâ€¦ http://instagram.com/p/YGEt5JC6JM/   \n",
       "2          We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu   \n",
       "3                     I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/   \n",
       "4         What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!   \n",
       "\n",
       "                                                                                               tweet_lst_clean  \\\n",
       "0  [fingerprint, pregnancy, test, android, aps, beautiful, cute, health, igers, iphoneonly, iphonesia, iphone]   \n",
       "1                         [finaly, transparant, silicon, case, thanks, uncle, yay, sony, xperia, sonyexperias]   \n",
       "2                         [love, would, go, talk, makemories, unplug, relax, iphone, smartphone, wifi, conect]   \n",
       "3                                               [wired, know, george, made, way, iphone, cute, daventry, home]   \n",
       "4                              [amazing, service, aple, wil, even, talk, question, unles, pay, stupid, suport]   \n",
       "\n",
       "                                                                                      tweet_clean  \\\n",
       "0  fingerprint pregnancy test android aps beautiful cute health igers iphoneonly iphonesia iphone   \n",
       "1                       finaly transparant silicon case thanks uncle yay sony xperia sonyexperias   \n",
       "2                        love would go talk makemories unplug relax iphone smartphone wifi conect   \n",
       "3                                            wired know george made way iphone cute daventry home   \n",
       "4                             amazing service aple wil even talk question unles pay stupid suport   \n",
       "\n",
       "                                                                                                        hashtags_lst  \\\n",
       "0  [#fingerprint, #Pregnancy, #android, #apps, #beautiful, #cute, #health, #igers, #iphoneonly, #iphonesia, #iphone]   \n",
       "1                                                                         [#yay, #Sony, #Xperia, #S, #sonyexperiasâ€¦]   \n",
       "2                                  [#talk, #makememories, #unplug, #relax, #iphone, #smartphone, #wifi, #connect...]   \n",
       "3                                                                                 [#iphone, #cute, #daventry, #home]   \n",
       "4                                                                                                                 []   \n",
       "\n",
       "                                                                                                hashtags  \n",
       "0  #fingerprint #Pregnancy #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone  \n",
       "1                                                                   #yay #Sony #Xperia #S #sonyexperiasâ€¦  \n",
       "2                               #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect...  \n",
       "3                                                                          #iphone #cute #daventry #home  \n",
       "4                                                                                                         "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticons and emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:26:16.704935Z",
     "start_time": "2020-09-01T01:26:16.696924Z"
    }
   },
   "outputs": [],
   "source": [
    "%run emoticons.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:26:18.032256Z",
     "start_time": "2020-09-01T01:26:18.013627Z"
    }
   },
   "outputs": [],
   "source": [
    "%run emojis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:26:18.874302Z",
     "start_time": "2020-09-01T01:26:18.854485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Happy_face_smiley Happy_face_smiley\n",
      "Thanks to my uncle Happy_face_or_smiley #yay\n"
     ]
    }
   ],
   "source": [
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "text1 = \"Hello :-) :-)\"\n",
    "text2 = \"Thanks to my uncle :) #yay\"\n",
    "\n",
    "print(convert_emoticons(text1))\n",
    "print(convert_emoticons(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:26:21.018457Z",
     "start_time": "2020-09-01T01:26:20.901607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game is on fire'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "text = \"game is on ðŸ”¥\"\n",
    "convert_emojis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:26:22.622012Z",
     "start_time": "2020-09-01T01:26:22.589964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['retwet', 'textnum', 'yes', 'god', 'wwxycom', 'amazing']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_text_emoji(text):\n",
    "    \"\"\"\n",
    "    Do a basic text processing.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    text : string\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    This function returns pandas series having one list\n",
    "    with clean text.\n",
    "    1: split combined text\n",
    "    2: lowercase\n",
    "    3: expand apostrophes\n",
    "    4: remove punctuation\n",
    "    5: remove digits\n",
    "    6: remove repeated substring\n",
    "    7: remove stop words\n",
    "    8: lemmatize\n",
    "\n",
    "    Example:\n",
    "    ========\n",
    "    import re\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    \n",
    "    text = \"I'm typing text2num! areYou ? If yesyes say yes pals!\"\n",
    "    process_text(text)\n",
    "    # ['typing', 'textnum', 'yes', 'say', 'yes', 'pal']\n",
    "\n",
    "    \"\"\"\n",
    "    s = pd.Series([text])\n",
    "    \n",
    "    # step: expand emoticons and emojis\n",
    "    s = s.apply(convert_emoticons)\n",
    "    s = s.apply(convert_emojis)\n",
    "\n",
    "    # step: Split combined words areYou ==> are You\n",
    "    #s = s.apply(lambda x: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',x))\n",
    "\n",
    "    # step: lowercase\n",
    "    s = s.str.lower()\n",
    "    \n",
    "    # step: remove ellipsis\n",
    "    #s = s.str.replace(r'(\\w)\\u2026+',r'\\1',regex=True)\n",
    "    s = s.str.replace(r'â€¦+',r'')\n",
    "\n",
    "    # step: remove url\n",
    "    #s = s.str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    s = pd.Series([' '.join(y for y in x.split() if not is_url(y)) for x in s])\n",
    "\n",
    "    # step: expand apostrophes\n",
    "    map_apos = {\n",
    "        \"you're\": 'you are',\n",
    "        \"i'm\": 'i am',\n",
    "        \"he's\": 'he is',\n",
    "        \"she's\": 'she is',\n",
    "        \"it's\": 'it is',\n",
    "        \"they're\": 'they are',\n",
    "        \"can't\": 'can not',\n",
    "        \"couldn't\": 'could not',\n",
    "        \"don't\": 'do not',\n",
    "        \"don;t\": 'do not',\n",
    "        \"didn't\": 'did not',\n",
    "        \"doesn't\": 'does not',\n",
    "        \"isn't\": 'is not',\n",
    "        \"wasn't\": 'was not',\n",
    "        \"aren't\": 'are not',\n",
    "        \"weren't\": 'were not',\n",
    "        \"won't\": 'will not',\n",
    "        \"wouldn't\": 'would not',\n",
    "        \"hasn't\": 'has not',\n",
    "        \"haven't\": 'have not',\n",
    "        \"what's\": 'what is',\n",
    "        \"that's\": 'that is',\n",
    "    }\n",
    "\n",
    "    sa = pd.Series(s.str.split()[0])\n",
    "    sb = sa.map(map_apos).fillna(sa)\n",
    "    sentence = sb.str.cat(sep=' ')\n",
    "    s = pd.Series([sentence])\n",
    "    \n",
    "    # step: expand shortcuts\n",
    "    shortcuts = {'u': 'you', 'y': 'why', 'r': 'are',\n",
    "                 'doin': 'doing', 'hw': 'how',\n",
    "                 'k': 'okay', 'm': 'am', 'b4': 'before',\n",
    "                 'idc': \"i do not care\", 'ty': 'thankyou',\n",
    "                 'wlcm': 'welcome', 'bc': 'because',\n",
    "                 '<3': 'love', 'xoxo': 'love',\n",
    "                 'ttyl': 'talk to you later', 'gr8': 'great',\n",
    "                 'bday': 'birthday', 'awsm': 'awesome',\n",
    "                 'gud': 'good', 'h8': 'hate',\n",
    "                 'lv': 'love', 'dm': 'direct message',\n",
    "                 'rt': 'retweet', 'wtf': 'hate',\n",
    "                 'idgaf': 'hate','irl': 'in real life',\n",
    "                 'yolo': 'you only live once'}\n",
    "\n",
    "    sa = pd.Series(s.str.split()[0])\n",
    "    sb = sa.map(shortcuts).fillna(sa)\n",
    "    sentence = sb.str.cat(sep=' ')\n",
    "    s = pd.Series([sentence])\n",
    "\n",
    "    # step: remove punctuation\n",
    "    s = s.str.translate(str.maketrans(' ',' ',\n",
    "                                        string.punctuation))\n",
    "    # step: remove digits\n",
    "    s = s.str.translate(str.maketrans(' ', ' ', '\\n'))\n",
    "    s = s.str.translate(str.maketrans(' ', ' ', string.digits))\n",
    "\n",
    "    # step: remove repeated substring yesyes ==> yes\n",
    "    s = s.str.replace(r'(\\w+)\\1',r'\\1',regex=True)\n",
    "\n",
    "    # step: remove stop words\n",
    "    stop = set(stopwords.words('English'))\n",
    "    extra_stop_words = ['...']\n",
    "    stop.update(extra_stop_words) # inplace operation\n",
    "    s = s.str.split()\n",
    "    s = s.apply(lambda x: [I for I in x if I not in stop])\n",
    "\n",
    "    # step: convert word to base form or lemmatize\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    s = s.apply(lambda lst: [lemmatizer.lemmatize(word) \n",
    "                               for word in lst])\n",
    "\n",
    "    return s.to_numpy()[0]\n",
    "\n",
    "text = \"rt text2num! yesyes gud www.xy.com amazing\"\n",
    "process_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:56:11.222656Z",
     "start_time": "2020-09-01T01:56:10.621471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_lst_clean_emoji</th>\n",
       "      <th>tweet_clean_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "      <td>[fingerprint, pregnancy, test, android, aps, beautiful, cute, health, igers, iphoneonly, iphonesia, iphone]</td>\n",
       "      <td>fingerprint pregnancy test android aps beautiful cute health igers iphoneonly iphonesia iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperiasâ€¦ http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "      <td>[finaly, transparant, silicon, case, thanks, uncle, hapyfaceorsmiley, yay, sony, xperia, sonyexperias]</td>\n",
       "      <td>finaly transparant silicon case thanks uncle hapyfaceorsmiley yay sony xperia sonyexperias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu</td>\n",
       "      <td>[love, would, go, talk, makemories, unplug, relax, iphone, smartphone, wifi, conect]</td>\n",
       "      <td>love would go talk makemories unplug relax iphone smartphone wifi conect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/</td>\n",
       "      <td>[wired, know, george, made, way, winkorsmirk, iphone, cute, daventry, home]</td>\n",
       "      <td>wired know george made way winkorsmirk iphone cute daventry home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!</td>\n",
       "      <td>[amazing, service, aple, wil, even, talk, question, unles, pay, stupid, suport]</td>\n",
       "      <td>amazing service aple wil even talk question unles pay stupid suport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id  label  \\\n",
       "0      0   1    0.0   \n",
       "1      1   2    0.0   \n",
       "2      2   3    0.0   \n",
       "3      3   4    0.0   \n",
       "4      4   5    1.0   \n",
       "\n",
       "                                                                                                                                 tweet  \\\n",
       "0     #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone   \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperiasâ€¦ http://instagram.com/p/YGEt5JC6JM/   \n",
       "2          We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu   \n",
       "3                     I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/   \n",
       "4         What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!   \n",
       "\n",
       "                                                                                         tweet_lst_clean_emoji  \\\n",
       "0  [fingerprint, pregnancy, test, android, aps, beautiful, cute, health, igers, iphoneonly, iphonesia, iphone]   \n",
       "1       [finaly, transparant, silicon, case, thanks, uncle, hapyfaceorsmiley, yay, sony, xperia, sonyexperias]   \n",
       "2                         [love, would, go, talk, makemories, unplug, relax, iphone, smartphone, wifi, conect]   \n",
       "3                                  [wired, know, george, made, way, winkorsmirk, iphone, cute, daventry, home]   \n",
       "4                              [amazing, service, aple, wil, even, talk, question, unles, pay, stupid, suport]   \n",
       "\n",
       "                                                                                tweet_clean_emoji  \n",
       "0  fingerprint pregnancy test android aps beautiful cute health igers iphoneonly iphonesia iphone  \n",
       "1      finaly transparant silicon case thanks uncle hapyfaceorsmiley yay sony xperia sonyexperias  \n",
       "2                        love would go talk makemories unplug relax iphone smartphone wifi conect  \n",
       "3                                wired know george made way winkorsmirk iphone cute daventry home  \n",
       "4                             amazing service aple wil even talk question unles pay stupid suport  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_features_emoji(df):\n",
    "    # we need to remove url first\n",
    "    df[mcle] = df[maincol].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    df[mcle] = df[mcle].apply(process_text_emoji)\n",
    "    df[mce] = df[mcle].str.join(' ')\n",
    "\n",
    "    return df\n",
    "\n",
    "add_features_emoji(df.head().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:27:02.019249Z",
     "start_time": "2020-09-01T01:26:54.702821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.3 ms, sys: 21.6 ms, total: 42.9 ms\n",
      "Wall time: 7.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This takes long time for full data\n",
    "df = parallelize_dataframe(df, add_features_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:27:03.772406Z",
     "start_time": "2020-09-01T01:27:03.752541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_lst_clean</th>\n",
       "      <th>tweet_clean</th>\n",
       "      <th>hashtags_lst</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_lst_clean_emoji</th>\n",
       "      <th>tweet_clean_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "      <td>[fingerprint, pregnancy, test, android, aps, b...</td>\n",
       "      <td>fingerprint pregnancy test android aps beautif...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[fingerprint, pregnancy, test, htpskepticalano...</td>\n",
       "      <td>fingerprint pregnancy test htpskepticalanoyedu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "      <td>[finaly, transparant, silicon, case, thanks, u...</td>\n",
       "      <td>finaly transparant silicon case thanks uncle y...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[finaly, transparant, silicon, case, thanks, u...</td>\n",
       "      <td>finaly transparant silicon case thanks uncle h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "      <td>[love, would, go, talk, makemories, unplug, re...</td>\n",
       "      <td>love would go talk makemories unplug relax iph...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[love, would, go, talk, makemories, unplug, re...</td>\n",
       "      <td>love would go talk makemories unplug relax iph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "      <td>[wired, know, george, made, way, iphone, cute,...</td>\n",
       "      <td>wired know george made way iphone cute daventr...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[wired, know, george, made, way, winkorsmirk, ...</td>\n",
       "      <td>wired know george made way winkorsmirk iphone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "      <td>[amazing, service, aple, wil, even, talk, ques...</td>\n",
       "      <td>amazing service aple wil even talk question un...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[amazing, service, aple, wil, even, talk, ques...</td>\n",
       "      <td>amazing service aple wil even talk question un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id  label                                              tweet  \\\n",
       "0      0   1    0.0  #fingerprint #Pregnancy Test https://goo.gl/h1...   \n",
       "1      1   2    0.0  Finally a transparant silicon case ^^ Thanks t...   \n",
       "2      2   3    0.0  We love this! Would you go? #talk #makememorie...   \n",
       "3      3   4    0.0  I'm wired I know I'm George I was made that wa...   \n",
       "4      4   5    1.0  What amazing service! Apple won't even talk to...   \n",
       "\n",
       "                                     tweet_lst_clean  \\\n",
       "0  [fingerprint, pregnancy, test, android, aps, b...   \n",
       "1  [finaly, transparant, silicon, case, thanks, u...   \n",
       "2  [love, would, go, talk, makemories, unplug, re...   \n",
       "3  [wired, know, george, made, way, iphone, cute,...   \n",
       "4  [amazing, service, aple, wil, even, talk, ques...   \n",
       "\n",
       "                                         tweet_clean hashtags_lst hashtags  \\\n",
       "0  fingerprint pregnancy test android aps beautif...           []            \n",
       "1  finaly transparant silicon case thanks uncle y...           []            \n",
       "2  love would go talk makemories unplug relax iph...           []            \n",
       "3  wired know george made way iphone cute daventr...           []            \n",
       "4  amazing service aple wil even talk question un...           []            \n",
       "\n",
       "                               tweet_lst_clean_emoji  \\\n",
       "0  [fingerprint, pregnancy, test, htpskepticalano...   \n",
       "1  [finaly, transparant, silicon, case, thanks, u...   \n",
       "2  [love, would, go, talk, makemories, unplug, re...   \n",
       "3  [wired, know, george, made, way, winkorsmirk, ...   \n",
       "4  [amazing, service, aple, wil, even, talk, ques...   \n",
       "\n",
       "                                   tweet_clean_emoji  \n",
       "0  fingerprint pregnancy test htpskepticalanoyedu...  \n",
       "1  finaly transparant silicon case thanks uncle h...  \n",
       "2  love would go talk makemories unplug relax iph...  \n",
       "3  wired know george made way winkorsmirk iphone ...  \n",
       "4  amazing service aple wil even talk question un...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:27:05.784488Z",
     "start_time": "2020-09-01T01:27:05.780959Z"
    }
   },
   "outputs": [],
   "source": [
    "note = \"\"\"\n",
    "Look the clean tweet properly:\n",
    "- look for url links\n",
    "- look for ellipsis e.g. #sonyexperiasâ€¦\n",
    "- convert emoji and emoticons eg. :) with library emot\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Features Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:27:09.034923Z",
     "start_time": "2020-09-01T01:27:09.021695Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_text_features(df):\n",
    "    # total\n",
    "    df['total_length'] = df[maincol].apply(len)\n",
    "\n",
    "    # num of word and sentence\n",
    "    df['num_words'] = df[maincol].apply(lambda x: len(x.split()))\n",
    "\n",
    "    df['num_sent']=df[maincol].apply(lambda x: \n",
    "                                len(re.findall(\"\\n\",str(x)))+1)\n",
    "\n",
    "    df['num_unique_words'] = df[maincol].apply(\n",
    "        lambda x: len(set(w for w in x.split())))\n",
    "\n",
    "    df[\"num_words_title\"] = df[maincol].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "    df['num_uppercase'] = df[maincol].apply(\n",
    "        lambda x: sum(1 for c in x if c.isupper()))\n",
    "\n",
    "    # num of certain characters ! ? . @\n",
    "    df['num_exclamation_marks'] = df[maincol].apply(lambda x: x.count('!'))\n",
    "\n",
    "    df['num_question_marks'] = df[maincol].apply(lambda x: x.count('?'))\n",
    "\n",
    "    df['num_punctuation'] = df[maincol].apply(\n",
    "        lambda x: sum(x.count(w) for w in '.,;:'))\n",
    "\n",
    "    df['num_symbols'] = df[maincol].apply(\n",
    "        lambda x: sum(x.count(w) for w in '*&$%'))\n",
    "    \n",
    "    df['num_digits'] = df[maincol].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "    # average\n",
    "    df[\"avg_word_len\"] = df[maincol].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    df['avg_uppercase'] = df.apply(\n",
    "        lambda row: float(row['num_uppercase'])/float(row['total_length']),\n",
    "                                    axis=1)\n",
    "    \n",
    "    df['avg_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:27:10.622720Z",
     "start_time": "2020-09-01T01:27:10.471492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 ms, sys: 23.4 ms, total: 47.7 ms\n",
      "Wall time: 143 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = parallelize_dataframe(df, create_text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T01:27:28.883554Z",
     "start_time": "2020-09-01T01:27:28.855696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_lst_clean</th>\n",
       "      <th>tweet_clean</th>\n",
       "      <th>hashtags_lst</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_lst_clean_emoji</th>\n",
       "      <th>tweet_clean_emoji</th>\n",
       "      <th>...</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>num_uppercase</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_punctuation</th>\n",
       "      <th>num_symbols</th>\n",
       "      <th>num_digits</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>avg_uppercase</th>\n",
       "      <th>avg_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "      <td>[fingerprint, pregnancy, test, android, aps, b...</td>\n",
       "      <td>fingerprint pregnancy test android aps beautif...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[fingerprint, pregnancy, test, htpskepticalano...</td>\n",
       "      <td>fingerprint pregnancy test htpskepticalanoyedu...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.923077</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "      <td>[finaly, transparant, silicon, case, thanks, u...</td>\n",
       "      <td>finaly transparant silicon case thanks uncle y...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[finaly, transparant, silicon, case, thanks, u...</td>\n",
       "      <td>finaly transparant silicon case thanks uncle h...</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.764706</td>\n",
       "      <td>0.091603</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "      <td>[love, would, go, talk, makemories, unplug, re...</td>\n",
       "      <td>love would go talk makemories unplug relax iph...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[love, would, go, talk, makemories, unplug, re...</td>\n",
       "      <td>love would go talk makemories unplug relax iph...</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.266667</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "      <td>[wired, know, george, made, way, iphone, cute,...</td>\n",
       "      <td>wired know george made way iphone cute daventr...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[wired, know, george, made, way, winkorsmirk, ...</td>\n",
       "      <td>wired know george made way winkorsmirk iphone ...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.647059</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "      <td>[amazing, service, aple, wil, even, talk, ques...</td>\n",
       "      <td>amazing service aple wil even talk question un...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[amazing, service, aple, wil, even, talk, ques...</td>\n",
       "      <td>amazing service aple wil even talk question un...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.434783</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.956522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id  label                                              tweet  \\\n",
       "0      0   1    0.0  #fingerprint #Pregnancy Test https://goo.gl/h1...   \n",
       "1      1   2    0.0  Finally a transparant silicon case ^^ Thanks t...   \n",
       "2      2   3    0.0  We love this! Would you go? #talk #makememorie...   \n",
       "3      3   4    0.0  I'm wired I know I'm George I was made that wa...   \n",
       "4      4   5    1.0  What amazing service! Apple won't even talk to...   \n",
       "\n",
       "                                     tweet_lst_clean  \\\n",
       "0  [fingerprint, pregnancy, test, android, aps, b...   \n",
       "1  [finaly, transparant, silicon, case, thanks, u...   \n",
       "2  [love, would, go, talk, makemories, unplug, re...   \n",
       "3  [wired, know, george, made, way, iphone, cute,...   \n",
       "4  [amazing, service, aple, wil, even, talk, ques...   \n",
       "\n",
       "                                         tweet_clean hashtags_lst hashtags  \\\n",
       "0  fingerprint pregnancy test android aps beautif...           []            \n",
       "1  finaly transparant silicon case thanks uncle y...           []            \n",
       "2  love would go talk makemories unplug relax iph...           []            \n",
       "3  wired know george made way iphone cute daventr...           []            \n",
       "4  amazing service aple wil even talk question un...           []            \n",
       "\n",
       "                               tweet_lst_clean_emoji  \\\n",
       "0  [fingerprint, pregnancy, test, htpskepticalano...   \n",
       "1  [finaly, transparant, silicon, case, thanks, u...   \n",
       "2  [love, would, go, talk, makemories, unplug, re...   \n",
       "3  [wired, know, george, made, way, winkorsmirk, ...   \n",
       "4  [amazing, service, aple, wil, even, talk, ques...   \n",
       "\n",
       "                                   tweet_clean_emoji  ...  num_words_title  \\\n",
       "0  fingerprint pregnancy test htpskepticalanoyedu...  ...                2   \n",
       "1  finaly transparant silicon case thanks uncle h...  ...                5   \n",
       "2  love would go talk makemories unplug relax iph...  ...                2   \n",
       "3  wired know george made way winkorsmirk iphone ...  ...                3   \n",
       "4  amazing service aple wil even talk question un...  ...                4   \n",
       "\n",
       "   num_uppercase  num_exclamation_marks  num_question_marks  num_punctuation  \\\n",
       "0              5                      0                   0                2   \n",
       "1             12                      0                   0                3   \n",
       "2              6                      1                   1                5   \n",
       "3              7                      0                   0                3   \n",
       "4              4                      2                   0                1   \n",
       "\n",
       "   num_symbols  num_digits  avg_word_len  avg_uppercase  avg_unique  \n",
       "0            0           0      8.923077       0.039062    1.000000  \n",
       "1            0           0      6.764706       0.091603    1.000000  \n",
       "2            0           0      7.266667       0.048780    1.000000  \n",
       "3            0           0      5.647059       0.062500    0.882353  \n",
       "4            1           0      4.434783       0.032258    0.956522  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T00:57:29.157949Z",
     "start_time": "2020-09-01T00:57:29.079908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sentiment_analysis_data_processing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sentiment_analysis_data_processing.py\n",
    "\n",
    "# load the path\n",
    "import sys\n",
    "sys.path.append('/Users/poudel/opt/miniconda3/envs/nlp/lib/python3.7/site-packages')\n",
    "\n",
    "# load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "import multiprocessing as mp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "# Load the data\n",
    "df_train_raw = pd.read_csv('../data/raw/train.csv')\n",
    "df_test_raw = pd.read_csv('../data/raw/test.csv')\n",
    "df = df_train_raw.append(df_test_raw)\n",
    "df = df.reset_index()\n",
    "\n",
    "# Variables\n",
    "target = 'label'\n",
    "maincol = 'tweet'\n",
    "mc = maincol + '_clean'\n",
    "mcl = maincol + '_lst_clean'\n",
    "mce = mc + '_emoji'\n",
    "mcle = mcl + '_emoji'\n",
    "\n",
    "# ==================== Useful functions ==============\n",
    "def parallelize_dataframe(df, func):\n",
    "    ncores = mp.cpu_count()\n",
    "    df_split = np.array_split(df, ncores)\n",
    "    pool = mp.Pool(ncores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def is_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "#================== Text processing =================\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Do a basic text processing.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    text : string\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    This function returns pandas series having one list\n",
    "    with clean text.\n",
    "    1: split combined text\n",
    "    2: lowercase\n",
    "    3: expand apostrophes\n",
    "    4: remove punctuation\n",
    "    5: remove digits\n",
    "    6: remove repeated substring\n",
    "    7: remove stop words\n",
    "    8: lemmatize\n",
    "\n",
    "    Example:\n",
    "    ========\n",
    "    import re\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    \n",
    "    text = \"I'm typing text2num! areYou ? If yesyes say yes pals!\"\n",
    "    process_text(text)\n",
    "    # ['typing', 'textnum', 'yes', 'say', 'yes', 'pal']\n",
    "\n",
    "    \"\"\"\n",
    "    s = pd.Series([text])\n",
    "    \n",
    "    # step: Split combined words areYou ==> are You\n",
    "    #s = s.apply(lambda x: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',x))\n",
    "\n",
    "    # step: lowercase\n",
    "    s = s.str.lower()\n",
    "    \n",
    "    # step: remove ellipsis\n",
    "    #s = s.str.replace(r'(\\w)\\u2026+',r'\\1',regex=True)\n",
    "    s = s.str.replace(r'â€¦+',r'')\n",
    "\n",
    "    # step: remove url\n",
    "    #s = s.str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    s = pd.Series([' '.join(y for y in x.split() if not is_url(y)) for x in s])\n",
    "\n",
    "    # step: expand apostrophes\n",
    "    map_apos = {\n",
    "        \"you're\": 'you are',\n",
    "        \"i'm\": 'i am',\n",
    "        \"he's\": 'he is',\n",
    "        \"she's\": 'she is',\n",
    "        \"it's\": 'it is',\n",
    "        \"they're\": 'they are',\n",
    "        \"can't\": 'can not',\n",
    "        \"couldn't\": 'could not',\n",
    "        \"don't\": 'do not',\n",
    "        \"don;t\": 'do not',\n",
    "        \"didn't\": 'did not',\n",
    "        \"doesn't\": 'does not',\n",
    "        \"isn't\": 'is not',\n",
    "        \"wasn't\": 'was not',\n",
    "        \"aren't\": 'are not',\n",
    "        \"weren't\": 'were not',\n",
    "        \"won't\": 'will not',\n",
    "        \"wouldn't\": 'would not',\n",
    "        \"hasn't\": 'has not',\n",
    "        \"haven't\": 'have not',\n",
    "        \"what's\": 'what is',\n",
    "        \"that's\": 'that is',\n",
    "    }\n",
    "\n",
    "    sa = pd.Series(s.str.split()[0])\n",
    "    sb = sa.map(map_apos).fillna(sa)\n",
    "    sentence = sb.str.cat(sep=' ')\n",
    "    s = pd.Series([sentence])\n",
    "    \n",
    "    # step: expand shortcuts\n",
    "    shortcuts = {'u': 'you', 'y': 'why', 'r': 'are',\n",
    "                 'doin': 'doing', 'hw': 'how',\n",
    "                 'k': 'okay', 'm': 'am', 'b4': 'before',\n",
    "                 'idc': \"i do not care\", 'ty': 'thankyou',\n",
    "                 'wlcm': 'welcome', 'bc': 'because',\n",
    "                 '<3': 'love', 'xoxo': 'love',\n",
    "                 'ttyl': 'talk to you later', 'gr8': 'great',\n",
    "                 'bday': 'birthday', 'awsm': 'awesome',\n",
    "                 'gud': 'good', 'h8': 'hate',\n",
    "                 'lv': 'love', 'dm': 'direct message',\n",
    "                 'rt': 'retweet', 'wtf': 'hate',\n",
    "                 'idgaf': 'hate','irl': 'in real life',\n",
    "                 'yolo': 'you only live once'}\n",
    "\n",
    "    sa = pd.Series(s.str.split()[0])\n",
    "    sb = sa.map(shortcuts).fillna(sa)\n",
    "    sentence = sb.str.cat(sep=' ')\n",
    "    s = pd.Series([sentence])\n",
    "\n",
    "    # step: remove punctuation\n",
    "    s = s.str.translate(str.maketrans(' ',' ',\n",
    "                                        string.punctuation))\n",
    "    # step: remove digits\n",
    "    s = s.str.translate(str.maketrans(' ', ' ', '\\n'))\n",
    "    s = s.str.translate(str.maketrans(' ', ' ', string.digits))\n",
    "\n",
    "    # step: remove repeated substring yesyes ==> yes\n",
    "    s = s.str.replace(r'(\\w+)\\1',r'\\1',regex=True)\n",
    "\n",
    "    # step: remove stop words\n",
    "    stop = set(stopwords.words('English'))\n",
    "    extra_stop_words = ['...']\n",
    "    stop.update(extra_stop_words) # inplace operation\n",
    "    s = s.str.split()\n",
    "    s = s.apply(lambda x: [I for I in x if I not in stop])\n",
    "\n",
    "    # step: convert word to base form or lemmatize\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    s = s.apply(lambda lst: [lemmatizer.lemmatize(word) \n",
    "                               for word in lst])\n",
    "\n",
    "    return s.to_numpy()[0]\n",
    "\n",
    "def add_features(df):\n",
    "    df[mcl] = df[maincol].apply(process_text)\n",
    "    df[mc] = df[mcl].str.join(' ')\n",
    "    df['hashtags_lst'] = df[maincol].str.findall(r'#.*?(?=\\s|$)')\n",
    "    \n",
    "    #df['hashtags'] = df[maincol].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "    \n",
    "    df['hashtags'] = df['hashtags_lst'].str.join(' ')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Creating clean tweet and hashtags ...\")\n",
    "df = parallelize_dataframe(df, add_features)\n",
    "\n",
    "#======================= Text Feature Generation =====\n",
    "def create_text_features(df):\n",
    "    # total\n",
    "    df['total_length'] = df[maincol].apply(len)\n",
    "\n",
    "    # num of word and sentence\n",
    "    df['num_words'] = df[maincol].apply(lambda x: len(x.split()))\n",
    "\n",
    "    df['num_sent']=df[maincol].apply(lambda x: \n",
    "                                len(re.findall(\"\\n\",str(x)))+1)\n",
    "\n",
    "    df['num_unique_words'] = df[maincol].apply(\n",
    "        lambda x: len(set(w for w in x.split())))\n",
    "\n",
    "    df[\"num_words_title\"] = df[maincol].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "    df['num_uppercase'] = df[maincol].apply(\n",
    "        lambda x: sum(1 for c in x if c.isupper()))\n",
    "\n",
    "    # num of certain characters ! ? . @\n",
    "    df['num_exclamation_marks'] = df[maincol].apply(lambda x: x.count('!'))\n",
    "\n",
    "    df['num_question_marks'] = df[maincol].apply(lambda x: x.count('?'))\n",
    "\n",
    "    df['num_punctuation'] = df[maincol].apply(\n",
    "        lambda x: sum(x.count(w) for w in '.,;:'))\n",
    "\n",
    "    df['num_symbols'] = df[maincol].apply(\n",
    "        lambda x: sum(x.count(w) for w in '*&$%'))\n",
    "    \n",
    "    df['num_digits'] = df[maincol].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "    # average\n",
    "    df[\"avg_word_len\"] = df[maincol].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    df['avg_uppercase'] = df.apply(\n",
    "        lambda row: float(row['num_uppercase'])/float(row['total_length']),\n",
    "                                    axis=1)\n",
    "\n",
    "    df['avg_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Adding Text features ...\")\n",
    "df = parallelize_dataframe(df, create_text_features)\n",
    "\n",
    "#===================== Emoticons =====================\n",
    "from emoticons.py import *\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "#===================== Save clean data =========================\n",
    "df.to_csv('../data/processed/df_combined_clean.csv',index=False)\n",
    "\n",
    "time_taken = time.time() - time_start\n",
    "m,s = divmod(time_taken,60)\n",
    "print(f\"Data cleaning finished in {m} min {s:.2f} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T02:09:31.680928Z",
     "start_time": "2020-09-01T02:09:31.669058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sentiment_analysis_data_processing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sentiment_analysis_data_processing.py\n",
    "\n",
    "# load the path\n",
    "import sys\n",
    "sys.path.append('/Users/poudel/opt/miniconda3/envs/nlp/lib/python3.7/site-packages')\n",
    "\n",
    "# load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "import multiprocessing as mp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "# Load the data\n",
    "df_train_raw = pd.read_csv('../data/raw/train.csv')\n",
    "df_test_raw = pd.read_csv('../data/raw/test.csv')\n",
    "df = df_train_raw.append(df_test_raw)\n",
    "df = df.reset_index()\n",
    "\n",
    "# Variables\n",
    "target = 'label'\n",
    "maincol = 'tweet'\n",
    "mc = maincol + '_clean'\n",
    "mcl = maincol + '_lst_clean'\n",
    "mce = mc + '_emoji'\n",
    "mcle = mcl + '_emoji'\n",
    "\n",
    "# ==================== Useful functions ==============\n",
    "def parallelize_dataframe(df, func):\n",
    "    ncores = mp.cpu_count()\n",
    "    df_split = np.array_split(df, ncores)\n",
    "    pool = mp.Pool(ncores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def is_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "#================== Text processing =================\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Do a basic text processing.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    text : string\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    This function returns pandas series having one list\n",
    "    with clean text.\n",
    "    1: split combined text\n",
    "    2: lowercase\n",
    "    3: expand apostrophes\n",
    "    4: remove punctuation\n",
    "    5: remove digits\n",
    "    6: remove repeated substring\n",
    "    7: remove stop words\n",
    "    8: lemmatize\n",
    "\n",
    "    Example:\n",
    "    ========\n",
    "    import re\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    \n",
    "    text = \"I'm typing text2num! areYou ? If yesyes say yes pals!\"\n",
    "    process_text(text)\n",
    "    # ['typing', 'textnum', 'yes', 'say', 'yes', 'pal']\n",
    "\n",
    "    \"\"\"\n",
    "    s = pd.Series([text])\n",
    "    \n",
    "    # step: Split combined words areYou ==> are You\n",
    "    #s = s.apply(lambda x: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',x))\n",
    "\n",
    "    # step: lowercase\n",
    "    s = s.str.lower()\n",
    "    \n",
    "    # step: remove ellipsis\n",
    "    #s = s.str.replace(r'(\\w)\\u2026+',r'\\1',regex=True)\n",
    "    s = s.str.replace(r'â€¦+',r'')\n",
    "\n",
    "    # step: remove url\n",
    "    #s = s.str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    s = pd.Series([' '.join(y for y in x.split() if not is_url(y)) for x in s])\n",
    "\n",
    "    # step: expand apostrophes\n",
    "    map_apos = {\n",
    "        \"you're\": 'you are',\n",
    "        \"i'm\": 'i am',\n",
    "        \"he's\": 'he is',\n",
    "        \"she's\": 'she is',\n",
    "        \"it's\": 'it is',\n",
    "        \"they're\": 'they are',\n",
    "        \"can't\": 'can not',\n",
    "        \"couldn't\": 'could not',\n",
    "        \"don't\": 'do not',\n",
    "        \"don;t\": 'do not',\n",
    "        \"didn't\": 'did not',\n",
    "        \"doesn't\": 'does not',\n",
    "        \"isn't\": 'is not',\n",
    "        \"wasn't\": 'was not',\n",
    "        \"aren't\": 'are not',\n",
    "        \"weren't\": 'were not',\n",
    "        \"won't\": 'will not',\n",
    "        \"wouldn't\": 'would not',\n",
    "        \"hasn't\": 'has not',\n",
    "        \"haven't\": 'have not',\n",
    "        \"what's\": 'what is',\n",
    "        \"that's\": 'that is',\n",
    "    }\n",
    "\n",
    "    sa = pd.Series(s.str.split()[0])\n",
    "    sb = sa.map(map_apos).fillna(sa)\n",
    "    sentence = sb.str.cat(sep=' ')\n",
    "    s = pd.Series([sentence])\n",
    "    \n",
    "    # step: expand shortcuts\n",
    "    shortcuts = {'u': 'you', 'y': 'why', 'r': 'are',\n",
    "                 'doin': 'doing', 'hw': 'how',\n",
    "                 'k': 'okay', 'm': 'am', 'b4': 'before',\n",
    "                 'idc': \"i do not care\", 'ty': 'thankyou',\n",
    "                 'wlcm': 'welcome', 'bc': 'because',\n",
    "                 '<3': 'love', 'xoxo': 'love',\n",
    "                 'ttyl': 'talk to you later', 'gr8': 'great',\n",
    "                 'bday': 'birthday', 'awsm': 'awesome',\n",
    "                 'gud': 'good', 'h8': 'hate',\n",
    "                 'lv': 'love', 'dm': 'direct message',\n",
    "                 'rt': 'retweet', 'wtf': 'hate',\n",
    "                 'idgaf': 'hate','irl': 'in real life',\n",
    "                 'yolo': 'you only live once'}\n",
    "\n",
    "    sa = pd.Series(s.str.split()[0])\n",
    "    sb = sa.map(shortcuts).fillna(sa)\n",
    "    sentence = sb.str.cat(sep=' ')\n",
    "    s = pd.Series([sentence])\n",
    "\n",
    "    # step: remove punctuation\n",
    "    s = s.str.translate(str.maketrans(' ',' ',\n",
    "                                        string.punctuation))\n",
    "    # step: remove digits\n",
    "    s = s.str.translate(str.maketrans(' ', ' ', '\\n'))\n",
    "    s = s.str.translate(str.maketrans(' ', ' ', string.digits))\n",
    "\n",
    "    # step: remove repeated substring yesyes ==> yes\n",
    "    s = s.str.replace(r'(\\w+)\\1',r'\\1',regex=True)\n",
    "\n",
    "    # step: remove stop words\n",
    "    stop = set(stopwords.words('English'))\n",
    "    extra_stop_words = ['...']\n",
    "    stop.update(extra_stop_words) # inplace operation\n",
    "    s = s.str.split()\n",
    "    s = s.apply(lambda x: [I for I in x if I not in stop])\n",
    "\n",
    "    # step: convert word to base form or lemmatize\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    s = s.apply(lambda lst: [lemmatizer.lemmatize(word) \n",
    "                               for word in lst])\n",
    "\n",
    "    return s.to_numpy()[0]\n",
    "\n",
    "def add_features(df):\n",
    "    df[mcl] = df[maincol].apply(process_text)\n",
    "    df[mc] = df[mcl].str.join(' ')\n",
    "    df['hashtags_lst'] = df[maincol].str.findall(r'#.*?(?=\\s|$)')\n",
    "    \n",
    "    #df['hashtags'] = df[maincol].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "    \n",
    "    df['hashtags'] = df['hashtags_lst'].str.join(' ')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Creating clean tweet and hashtags ...\")\n",
    "df = parallelize_dataframe(df, add_features)\n",
    "\n",
    "#======================= Text Feature Generation =====\n",
    "def create_text_features(df):\n",
    "    # total\n",
    "    df['total_length'] = df[maincol].apply(len)\n",
    "\n",
    "    # num of word and sentence\n",
    "    df['num_words'] = df[maincol].apply(lambda x: len(x.split()))\n",
    "\n",
    "    df['num_sent']=df[maincol].apply(lambda x: \n",
    "                                len(re.findall(\"\\n\",str(x)))+1)\n",
    "\n",
    "    df['num_unique_words'] = df[maincol].apply(\n",
    "        lambda x: len(set(w for w in x.split())))\n",
    "\n",
    "    df[\"num_words_title\"] = df[maincol].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "    df['num_uppercase'] = df[maincol].apply(\n",
    "        lambda x: sum(1 for c in x if c.isupper()))\n",
    "\n",
    "    # num of certain characters ! ? . @\n",
    "    df['num_exclamation_marks'] = df[maincol].apply(lambda x: x.count('!'))\n",
    "\n",
    "    df['num_question_marks'] = df[maincol].apply(lambda x: x.count('?'))\n",
    "\n",
    "    df['num_punctuation'] = df[maincol].apply(\n",
    "        lambda x: sum(x.count(w) for w in '.,;:'))\n",
    "\n",
    "    df['num_symbols'] = df[maincol].apply(\n",
    "        lambda x: sum(x.count(w) for w in '*&$%'))\n",
    "    \n",
    "    df['num_digits'] = df[maincol].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "    # average\n",
    "    df[\"avg_word_len\"] = df[maincol].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    df['avg_uppercase'] = df.apply(\n",
    "        lambda row: float(row['num_uppercase'])/float(row['total_length']),\n",
    "                                    axis=1)\n",
    "\n",
    "    df['avg_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Adding Text features ...\")\n",
    "df = parallelize_dataframe(df, create_text_features)\n",
    "\n",
    "#===================== Manipulating emoticons and emojis\n",
    "from emojis import *\n",
    "from emoticons import *\n",
    "\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "def process_text_emoji(text):\n",
    "    \"\"\"\n",
    "    Do a basic text processing.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    text : string\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    This function returns pandas series having one list\n",
    "    with clean text.\n",
    "    1: split combined text\n",
    "    2: lowercase\n",
    "    3: expand apostrophes\n",
    "    4: remove punctuation\n",
    "    5: remove digits\n",
    "    6: remove repeated substring\n",
    "    7: remove stop words\n",
    "    8: lemmatize\n",
    "\n",
    "    Example:\n",
    "    ========\n",
    "    import re\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    \n",
    "    text = \"I'm typing text2num! areYou ? If yesyes say yes pals!\"\n",
    "    process_text(text)\n",
    "    # ['typing', 'textnum', 'yes', 'say', 'yes', 'pal']\n",
    "\n",
    "    \"\"\"\n",
    "    s = pd.Series([text])\n",
    "    \n",
    "    # step: expand emoticons and emojis\n",
    "    s = s.apply(convert_emoticons)\n",
    "    s = s.apply(convert_emojis)\n",
    "\n",
    "    # step: Split combined words areYou ==> are You\n",
    "    #s = s.apply(lambda x: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',x))\n",
    "\n",
    "    # step: lowercase\n",
    "    s = s.str.lower()\n",
    "    \n",
    "    # step: remove ellipsis\n",
    "    #s = s.str.replace(r'(\\w)\\u2026+',r'\\1',regex=True)\n",
    "    s = s.str.replace(r'â€¦+',r'')\n",
    "\n",
    "    # step: remove url\n",
    "    #s = s.str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    s = pd.Series([' '.join(y for y in x.split() if not is_url(y)) for x in s])\n",
    "\n",
    "    # step: expand apostrophes\n",
    "    map_apos = {\n",
    "        \"you're\": 'you are',\n",
    "        \"i'm\": 'i am',\n",
    "        \"he's\": 'he is',\n",
    "        \"she's\": 'she is',\n",
    "        \"it's\": 'it is',\n",
    "        \"they're\": 'they are',\n",
    "        \"can't\": 'can not',\n",
    "        \"couldn't\": 'could not',\n",
    "        \"don't\": 'do not',\n",
    "        \"don;t\": 'do not',\n",
    "        \"didn't\": 'did not',\n",
    "        \"doesn't\": 'does not',\n",
    "        \"isn't\": 'is not',\n",
    "        \"wasn't\": 'was not',\n",
    "        \"aren't\": 'are not',\n",
    "        \"weren't\": 'were not',\n",
    "        \"won't\": 'will not',\n",
    "        \"wouldn't\": 'would not',\n",
    "        \"hasn't\": 'has not',\n",
    "        \"haven't\": 'have not',\n",
    "        \"what's\": 'what is',\n",
    "        \"that's\": 'that is',\n",
    "    }\n",
    "\n",
    "    sa = pd.Series(s.str.split()[0])\n",
    "    sb = sa.map(map_apos).fillna(sa)\n",
    "    sentence = sb.str.cat(sep=' ')\n",
    "    s = pd.Series([sentence])\n",
    "    \n",
    "    # step: expand shortcuts\n",
    "    shortcuts = {'u': 'you', 'y': 'why', 'r': 'are',\n",
    "                 'doin': 'doing', 'hw': 'how',\n",
    "                 'k': 'okay', 'm': 'am', 'b4': 'before',\n",
    "                 'idc': \"i do not care\", 'ty': 'thankyou',\n",
    "                 'wlcm': 'welcome', 'bc': 'because',\n",
    "                 '<3': 'love', 'xoxo': 'love',\n",
    "                 'ttyl': 'talk to you later', 'gr8': 'great',\n",
    "                 'bday': 'birthday', 'awsm': 'awesome',\n",
    "                 'gud': 'good', 'h8': 'hate',\n",
    "                 'lv': 'love', 'dm': 'direct message',\n",
    "                 'rt': 'retweet', 'wtf': 'hate',\n",
    "                 'idgaf': 'hate','irl': 'in real life',\n",
    "                 'yolo': 'you only live once'}\n",
    "\n",
    "    sa = pd.Series(s.str.split()[0])\n",
    "    sb = sa.map(shortcuts).fillna(sa)\n",
    "    sentence = sb.str.cat(sep=' ')\n",
    "    s = pd.Series([sentence])\n",
    "\n",
    "    # step: remove punctuation\n",
    "    s = s.str.translate(str.maketrans(' ',' ',\n",
    "                                        string.punctuation))\n",
    "    # step: remove digits\n",
    "    s = s.str.translate(str.maketrans(' ', ' ', '\\n'))\n",
    "    s = s.str.translate(str.maketrans(' ', ' ', string.digits))\n",
    "\n",
    "    # step: remove repeated substring yesyes ==> yes\n",
    "    s = s.str.replace(r'(\\w+)\\1',r'\\1',regex=True)\n",
    "\n",
    "    # step: remove stop words\n",
    "    stop = set(stopwords.words('English'))\n",
    "    extra_stop_words = ['...']\n",
    "    stop.update(extra_stop_words) # inplace operation\n",
    "    s = s.str.split()\n",
    "    s = s.apply(lambda x: [I for I in x if I not in stop])\n",
    "\n",
    "    # step: convert word to base form or lemmatize\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    s = s.apply(lambda lst: [lemmatizer.lemmatize(word) \n",
    "                               for word in lst])\n",
    "\n",
    "    return s.to_numpy()[0]\n",
    "\n",
    "def add_features_emoji(df):\n",
    "    # we need to remove url first\n",
    "    df[mcle] = df[maincol].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    df[mcle] = df[mcle].apply(process_text_emoji)\n",
    "    df[mce] = df[mcle].str.join(' ')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Adding Emoticons and emoji features ...\")\n",
    "df = parallelize_dataframe(df, add_features_emoji)\n",
    "\n",
    "#===================== Save clean data =========================\n",
    "df.to_csv('../data/processed/df_combined_clean.csv',index=False)\n",
    "\n",
    "time_taken = time.time() - time_start\n",
    "m,s = divmod(time_taken,60)\n",
    "print(f\"Data cleaning finished in {m} min {s:.2f} sec.\")\n",
    "\n",
    "# Data cleaning finished in 12.0 min 6.19 sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python37 (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
