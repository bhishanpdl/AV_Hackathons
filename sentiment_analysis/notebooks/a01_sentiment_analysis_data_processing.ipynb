{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Description</a></span></li><li><span><a href=\"#Load-the-libraries\" data-toc-modified-id=\"Load-the-libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load the libraries</a></span></li><li><span><a href=\"#Load-the-data\" data-toc-modified-id=\"Load-the-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load the data</a></span></li><li><span><a href=\"#Useful-Functions\" data-toc-modified-id=\"Useful-Functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Useful Functions</a></span></li><li><span><a href=\"#Text-Data-Processing\" data-toc-modified-id=\"Text-Data-Processing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Text Data Processing</a></span></li><li><span><a href=\"#Text-Features-Generation\" data-toc-modified-id=\"Text-Features-Generation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Text Features Generation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Twitter sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T16:14:41.971266Z",
     "start_time": "2020-08-31T16:14:41.967859Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/poudel/opt/miniconda3/envs/nlp/lib/python3.7/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T16:46:55.250498Z",
     "start_time": "2020-08-31T16:46:55.207906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('numpy', '1.17.5'), ('pandas', '1.0.5'), ('seaborn', '0.10.1'), ('sklearn', '0.23.1'), ('mlxtend', '0.17.0'), ('plotly_express', '0.4.1')]\n",
      "[('nltk', '3.4.4'), ('spacy', '2.2.3'), ('textblob', '0.15.3'), ('gensim', '3.8.3')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import mlxtend\n",
    "import plotly_express as px\n",
    "\n",
    "pd.options.plotting.backend = \"matplotlib\"\n",
    "pd.set_option('max_colwidth',500)\n",
    "\n",
    "import time,os,json,sys\n",
    "time_start_notebook = time.time()\n",
    "home = os.path.expanduser('~')\n",
    "SEED=100\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print([(x.__name__,x.__version__) for x in [np,pd,sns,sklearn,mlxtend,px]])\n",
    "\n",
    "\n",
    "#========= NLP\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import textblob\n",
    "import gensim\n",
    "import texthero\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print([(x.__name__,x.__version__) for x in [nltk,spacy,textblob,gensim]])\n",
    "\n",
    "#=======OTHERS\n",
    "import scipy\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T16:15:20.695546Z",
     "start_time": "2020-08-31T16:15:20.688345Z"
    }
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T16:39:12.548263Z",
     "start_time": "2020-08-31T16:39:12.505473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape df_train_raw: (7920, 3)\n",
      "shape df_test_raw: (1953, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train_raw = pd.read_csv('../data/raw/train.csv')\n",
    "df_test_raw = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "df = df_train_raw.append(df_test_raw)\n",
    "df = df.reset_index()\n",
    "\n",
    "print(f\"shape df_train_raw: {df_train_raw.shape}\")\n",
    "print(f\"shape df_test_raw: {df_test_raw.shape}\")\n",
    "\n",
    "df.head(2).append(df.tail(2))\n",
    "\n",
    "maincol = 'tweet'\n",
    "mc = maincol + '_clean'\n",
    "mcl = maincol + '_lst_clean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T16:54:40.330389Z",
     "start_time": "2020-08-31T16:54:40.326032Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "def parallelize_dataframe(df, func):\n",
    "    ncores = mp.cpu_count()\n",
    "    df_split = np.array_split(df, ncores)\n",
    "    pool = mp.Pool(ncores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T16:41:23.387785Z",
     "start_time": "2020-08-31T16:41:23.341538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['typing', 'textnum', 'areyou', 'yes', 'say', 'yes', 'pal']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Do a basic text processing.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    text : string\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    This function returns pandas series having one list\n",
    "    with clean text.\n",
    "    1: split combined text\n",
    "    2: lowercase\n",
    "    3: expand apostrophes\n",
    "    4: remove punctuation\n",
    "    5: remove digits\n",
    "    6: remove repeated substring\n",
    "    7: remove stop words\n",
    "    8: lemmatize\n",
    "\n",
    "    Example:\n",
    "    ========\n",
    "    import re\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    \n",
    "    text = \"I'm typing text2num! areYou ? If yesyes say yes pals!\"\n",
    "    process_text(text)\n",
    "    # ['typing', 'textnum', 'yes', 'say', 'yes', 'pal']\n",
    "\n",
    "    \"\"\"\n",
    "    ser = pd.Series([text])\n",
    "    \n",
    "    # step1: Split combined words areYou ==> are You\n",
    "    #s1 = ser.apply(lambda x: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',x))\n",
    "    s1 = ser\n",
    "\n",
    "    # step2: lowercase\n",
    "    s2 = s1.str.lower()\n",
    "\n",
    "    # step3: expand apostrophes\n",
    "    map_apos = {\n",
    "        \"you're\": 'you are',\n",
    "        \"i'm\": 'i am',\n",
    "        \"he's\": 'he is',\n",
    "        \"she's\": 'she is',\n",
    "        \"it's\": 'it is',\n",
    "        \"they're\": 'they are',\n",
    "        \"can't\": 'can not',\n",
    "        \"couldn't\": 'could not',\n",
    "        \"don't\": 'do not',\n",
    "        \"don;t\": 'do not',\n",
    "        \"didn't\": 'did not',\n",
    "        \"doesn't\": 'does not',\n",
    "        \"isn't\": 'is not',\n",
    "        \"wasn't\": 'was not',\n",
    "        \"aren't\": 'are not',\n",
    "        \"weren't\": 'were not',\n",
    "        \"won't\": 'will not',\n",
    "        \"wouldn't\": 'would not',\n",
    "        \"hasn't\": 'has not',\n",
    "        \"haven't\": 'have not',\n",
    "        \"what's\": 'what is',\n",
    "        \"that's\": 'that is',\n",
    "    }\n",
    "\n",
    "    s3a = pd.Series(s2.str.split()[0])\n",
    "    s3b = s3a.map(map_apos).fillna(s3a)\n",
    "    sentence = s3b.str.cat(sep=' ')\n",
    "    s3 = pd.Series([sentence])\n",
    "\n",
    "    # step4: remove punctuation\n",
    "    s4 = s3.str.translate(str.maketrans(' ',' ',\n",
    "                                        string.punctuation))\n",
    "    # step5: remove digits\n",
    "    s5 = s4.str.translate(str.maketrans(' ', ' ', '\\n'))\n",
    "    s5 = s5.str.translate(str.maketrans(' ', ' ', string.digits))\n",
    "\n",
    "    # step6: remove repeated substring yesyes ==> yes\n",
    "    s6 = s5.str.replace(r'(\\w+)\\1',r'\\1',regex=True)\n",
    "\n",
    "    # step7: remove stop words\n",
    "    stop = set(stopwords.words('English'))\n",
    "    extra_stop_words = ['...']\n",
    "    stop.update(extra_stop_words) # inplace operation\n",
    "    s7 = s6.str.split()\n",
    "    s7 = s7.apply(lambda x: [I for I in x if I not in stop])\n",
    "\n",
    "    # step8: convert word to base form or lemmatize\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    s8 = s7.apply(lambda lst: [lemmatizer.lemmatize(word) \n",
    "                               for word in lst])\n",
    "\n",
    "    return s8.to_numpy()[0]\n",
    "\n",
    "text = \"I'm typing text2num! areYou ? If yesyes say yes pals!\"\n",
    "# ['typing', 'textnum', 'yes', 'say', 'yes', 'pal']\n",
    "process_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T16:45:12.407449Z",
     "start_time": "2020-08-31T16:45:12.400746Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df[mcl] = df[maincol].apply(process_text)\n",
    "    df[mc] = df[mcl].str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T16:45:26.401412Z",
     "start_time": "2020-08-31T16:45:13.135156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 87.6 ms, sys: 54.4 ms, total: 142 ms\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = parallelize_dataframe(df, add_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T16:47:02.817430Z",
     "start_time": "2020-08-31T16:47:02.776584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_lst_clean</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "      <td>[fingerprint, pregnancy, test, htpsgoglhmfqv, android, aps, beautiful, cute, health, igers, iphoneonly, iphonesia, iphone]</td>\n",
       "      <td>fingerprint pregnancy test htpsgoglhmfqv android aps beautiful cute health igers iphoneonly iphonesia iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "      <td>[finaly, transparant, silicon, case, thanks, uncle, yay, sony, xperia, sonyexperias…, htpinstagramcompygetjcjm]</td>\n",
       "      <td>finaly transparant silicon case thanks uncle yay sony xperia sonyexperias… htpinstagramcompygetjcjm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu</td>\n",
       "      <td>[love, would, go, talk, makemories, unplug, relax, iphone, smartphone, wifi, conect, htpfbmenlsupcu]</td>\n",
       "      <td>love would go talk makemories unplug relax iphone smartphone wifi conect htpfbmenlsupcu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/</td>\n",
       "      <td>[wired, know, george, made, way, iphone, cute, daventry, home, htpinstagrampliujsk]</td>\n",
       "      <td>wired know george made way iphone cute daventry home htpinstagrampliujsk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!</td>\n",
       "      <td>[amazing, service, aple, wil, even, talk, question, unles, pay, stupid, suport]</td>\n",
       "      <td>amazing service aple wil even talk question unles pay stupid suport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id  label  \\\n",
       "0      0   1    0.0   \n",
       "1      1   2    0.0   \n",
       "2      2   3    0.0   \n",
       "3      3   4    0.0   \n",
       "4      4   5    1.0   \n",
       "\n",
       "                                                                                                                                 tweet  \\\n",
       "0     #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone   \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/   \n",
       "2          We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu   \n",
       "3                     I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/   \n",
       "4         What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!   \n",
       "\n",
       "                                                                                                              tweet_lst_clean  \\\n",
       "0  [fingerprint, pregnancy, test, htpsgoglhmfqv, android, aps, beautiful, cute, health, igers, iphoneonly, iphonesia, iphone]   \n",
       "1             [finaly, transparant, silicon, case, thanks, uncle, yay, sony, xperia, sonyexperias…, htpinstagramcompygetjcjm]   \n",
       "2                        [love, would, go, talk, makemories, unplug, relax, iphone, smartphone, wifi, conect, htpfbmenlsupcu]   \n",
       "3                                         [wired, know, george, made, way, iphone, cute, daventry, home, htpinstagrampliujsk]   \n",
       "4                                             [amazing, service, aple, wil, even, talk, question, unles, pay, stupid, suport]   \n",
       "\n",
       "                                                                                                    tweet_clean  \n",
       "0  fingerprint pregnancy test htpsgoglhmfqv android aps beautiful cute health igers iphoneonly iphonesia iphone  \n",
       "1           finaly transparant silicon case thanks uncle yay sony xperia sonyexperias… htpinstagramcompygetjcjm  \n",
       "2                       love would go talk makemories unplug relax iphone smartphone wifi conect htpfbmenlsupcu  \n",
       "3                                      wired know george made way iphone cute daventry home htpinstagrampliujsk  \n",
       "4                                           amazing service aple wil even talk question unles pay stupid suport  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T16:47:43.755655Z",
     "start_time": "2020-08-31T16:47:43.752540Z"
    }
   },
   "outputs": [],
   "source": [
    "# note: I see some ... I need to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Features Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T17:06:57.926061Z",
     "start_time": "2020-08-31T17:06:57.914248Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_text_features(df):\n",
    "    # total\n",
    "    df['total_length'] = df[maincol].apply(len)\n",
    "\n",
    "    # num of word and sentence\n",
    "    df['num_words'] = df[maincol].apply(lambda x: len(x.split()))\n",
    "\n",
    "    df['num_sent']=df[\"x_text\"].apply(lambda x: \n",
    "                                len(re.findall(\"\\n\",str(x)))+1)\n",
    "\n",
    "    df['num_unique_words'] = df[maincol].apply(\n",
    "        lambda x: len(set(w for w in x.split())))\n",
    "\n",
    "    df[\"num_words_title\"] = df[maincol].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "    df['num_uppercase'] = df[maincol].apply(\n",
    "        lambda x: sum(1 for c in x if c.isupper()))\n",
    "\n",
    "    # num of certain characters ! ? . @\n",
    "    df['num_exclamation_marks'] = df[maincol].apply(lambda x: x.count('!'))\n",
    "\n",
    "    df['num_question_marks'] = df[maincol].apply(lambda x: x.count('?'))\n",
    "\n",
    "    df['num_punctuation'] = df[maincol].apply(\n",
    "        lambda x: sum(x.count(w) for w in '.,;:'))\n",
    "\n",
    "    df['num_symbols'] = df[maincol].apply(\n",
    "        lambda x: sum(x.count(w) for w in '*&$%'))\n",
    "\n",
    "    # average\n",
    "    df[\"avg_word_len\"] = df[maincol].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    df['avg_uppercase'] = df.apply(\n",
    "        lambda row: float(row['num_uppercase'])/float(row['total_length']),\n",
    "                                    axis=1)\n",
    "    \n",
    "    df['avg_unique'] = df['num_unique_words'] / df['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python37 (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
